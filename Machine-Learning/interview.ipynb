{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Assumptions:\n",
    "        Linear relationship, normal distribution, no multi colinearity,\n",
    "        Error term must show constant varrience,\n",
    "        Heteroscedasticity vs homescedasticity\n",
    "\n",
    "Performance:\n",
    "        R2 = 1-(RSS/TSS)\n",
    "        RSS = y-y(pred)\n",
    "        TSS = y-y(mean)\n",
    "\n",
    "        adustedR2 = 1 - (1-R2)(N-1) / N-P-1\n",
    "Loss:\n",
    "        MSE, RMSE, MAE, Mean Absolute Percent Error\n",
    "\n",
    "GLobal minina, Gradient descent,\n",
    "best fit line, learning rate,\n",
    "\n",
    "\n",
    "Hyperparameter:\n",
    "        Regularization Type: Ridge Regression (L2 Regularization), Lasso Regression (L1 Regularization), Elastic Net\n",
    "        Learning Rate :\n",
    "        Fit Intercept:\n",
    "                Definition: A boolean parameter that indicates whether the intercept (bias term) should be added to the model.\n",
    "                Effect: If fit_intercept=True, the model includes an intercept term; otherwise, it assumes the data is already centered.\n",
    "                Typical approach: True for most cases, unless the data is preprocessed to have zero mean.\n",
    "        Number of Iterations (Max Iterations):\n",
    "                Definition: The maximum number of times the algorithm is allowed to iterate over the training data.\n",
    "                Effect: Setting this too low may prevent convergence, especially for complex models.\n",
    "                Setting it too high may lead to unnecessary computations.\n",
    "\n",
    "\n",
    "imp question:\n",
    "Corelation : It measures the strength or degree of relationship between two variables\n",
    "Regression: t measures how one variable affects another variable\n",
    "Ordinaly Least square:-This method finds the best fit line,\n",
    "known as regression line by minimizing the sum of square differences between the observed and predicted values.\n",
    "\n",
    "1. Ordinary Least Squares(Statistics domain):\n",
    "To implement this in Scikit-learn we have to use the LinearRegression() class.\n",
    "2. Gradient Descent(Calculus family):\n",
    "To implement this in Scikit-learn we have to use the SGDRegressor() class\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Equation:\n",
    "        1/(1-e-y) --> y=mx+c  sigmoid function\n",
    "Assumptions:\n",
    "        Linear relationship, normal distribution, no multi colinearity,\n",
    "        Error term must show constant varrience,\n",
    "\n",
    "Performance:\n",
    "        Accuracy = all correct / all observation\n",
    "        Presion = True Poitive out of tatal positive (TP+FP) by model\n",
    "        Recall =  True Positive out of total actual positive (TP+FN)  aslo know as sensitivity\n",
    "        Specifitty\n",
    "        F1 = Harmonic mean of Presicion and Recall\n",
    "\n",
    "TO Adjusts the threshold we use AUC ROC graph\n",
    "Loss:\n",
    "\n",
    "\n",
    "GLobal minina, Gradient descent,\n",
    "best fit line, learning rate,\n",
    "\n",
    "\n",
    "Hyperparameter:\n",
    "        Regularization Type: Ridge Regression (L2 Regularization), Lasso Regression (L1 Regularization), Elastic Net\n",
    "        Solver:liblinear, lbfgs,\n",
    "        Learning Rate (in case of gradient-based solvers)\n",
    "        Class Weights:\n",
    "                Definition: Adjusts weights inversely proportional to class frequencies to handle imbalanced datasets.\n",
    "                How it works: By setting the class_weight parameter to balanced, the algorithm assigns higher weights to less frequent classes, effectively balancing their impact during training.\n",
    "                Effect: Improves model performance on imbalanced datasets by reducing bias toward the majority class.\n",
    "        Number of Iterations (Max Iterations):\n",
    "                Definition: The maximum number of times the algorithm is allowed to iterate over the training data.\n",
    "                Effect: Setting this too low may prevent convergence, especially for complex models. Setting it too high may lead to unnecessary computations.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dicision Tree"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Works:\n",
    "\n",
    "Important term: root node, leaf node, entropy, information gain\n",
    "\n",
    "Hyperparameter:\n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# Initialize the Decision Tree Classifier with specific hyperparameters\n",
    "clf = DecisionTreeClassifier(\n",
    "    criterion='gini', or entropy\n",
    "    splitter='best', random\n",
    "    max_depth=5,\n",
    "    min_samples_split=4,\n",
    "    min_samples_leaf=2,\n",
    "    min_weight_fraction_leaf=0.0,\n",
    "    max_features='sqrt',\n",
    "    random_state=42,\n",
    "    max_leaf_nodes=10,\n",
    "    min_impurity_decrease=0.01,\n",
    "    class_weight='balanced',\n",
    "    ccp_alpha=0.01\n",
    ")\n",
    "\n",
    "# Fit the classifier on training data (X_train, y_train)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Initialize the Decision Tree Regressor with specific hyperparameters\n",
    "reg = DecisionTreeRegressor(\n",
    "    criterion='squared_error', \"friedman_mse\", \"absolute_error\"\n",
    "    splitter='best',random\n",
    "    max_depth=5,\n",
    "    min_samples_split=4,\n",
    "    min_samples_leaf=2,\n",
    "    min_weight_fraction_leaf=0.0,\n",
    "    max_features='sqrt',\n",
    "    random_state=42,\n",
    "    max_leaf_nodes=10,\n",
    "    min_impurity_decrease=0.01,\n",
    "    ccp_alpha=0.01\n",
    ")\n",
    "\n",
    "# Fit the regressor on training data (X_train, y_train)\n",
    "reg.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Bagging\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "When using bagging in machine learning, following best practices and tips can maximize its effectiveness:\n",
    "\n",
    "    Use bagging when you have high-variance models like decision trees to improve stability.\n",
    "    Combine bagging with cross-validation for a more reliable evaluation of your models.\n",
    "    Use random forests (bagged decision trees) as a powerful, ready-to-use bagging technique.\n",
    "    Set a higher n_estimators value like 100-200 when bagging to obtain maximum benefit.\n",
    "    Bagging is easily parallelized using n_jobs. Implement it across multiple CPUs/machines for faster training.\n",
    "    Since bagging relies on bootstrap sampling, ensure that each model is trained on a sufficiently diverse subset of the data.\n",
    "    Before aggregating, optimize each model's performance using GridSearchCV.\n",
    "    Good performance of individual models usually translates to better performance of the ensemble.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Boosting works by sequentially training a series of weak learners, with each new learner attempting to correct the errors made by the previous ones.\n",
    "Here's a detailed explanation of the steps involved in a boosting algorithm:\n",
    "\n",
    "    Initialize the Model:\n",
    "        Start with a weak learner (like a shallow decision tree) that is trained on the entire dataset.\n",
    "        This learner makes its predictions based on the data.\n",
    "\n",
    "    Calculate Errors:\n",
    "        After the initial learner makes its predictions, the error (difference between actual and predicted values) is calculated.\n",
    "        Instances that were incorrectly predicted by the current model are given more weight in the training of the next model.\n",
    "\n",
    "    Update Weights:\n",
    "        The boosting algorithm increases the weights of the misclassified data points.\n",
    "        This makes them more important in the training process of the next weak learner. Correctly classified instances might have their weights reduced, making them less influential.\n",
    "\n",
    "    Train a New Weak Learner:\n",
    "        A new weak learner is trained using the updated weights, focusing more on the misclassified instances from the previous step.\n",
    "\n",
    "    Combine Learners:\n",
    "        Each weak learner contributes to the final prediction based on its accuracy.\n",
    "        The models are combined in such a way that the more accurate models have a higher influence on the final prediction.\n",
    "        The process is repeated for a predefined number of iterations or until the performance reaches a satisfactory level.\n",
    "\n",
    "    Final Prediction:\n",
    "        In the end, the boosting algorithm combines all the weak learners' predictions into a single strong prediction.\n",
    "        In the case of classification, this could be a weighted majority vote; for regression, it might be a weighted average of predictions.\n",
    "\n",
    "\n",
    "\n",
    "Hyperprameter:\n",
    "    n_estimators: Number of boosting rounds (trees to build).\n",
    "    max_depth: Maximum depth of a tree (controls the model complexity).\n",
    "    learning_rate (or eta): Step size shrinkage to prevent overfitting (commonly set between 0.01 and 0.3).\n",
    "    subsample: Fraction of samples to be used for each tree (helps prevent overfitting, typically between 0.5 and 1).\n",
    "    colsample_bytree: Fraction of features to be used for each tree (column subsampling).\n",
    "    gamma: Minimum loss reduction required to make a split (higher values make the algorithm more conservative).\n",
    "    lambda and alpha: L2 and L1 regularization terms on weights (control complexity).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### SVM\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "c Hyperparameter -->> provide the generalization by introducing the misclassification to avoid overfitting\n",
    "Better generalization to unseen data\tMore focus on training accuracy (risk of overfitting)\n",
    "Tolerates noise in the training data\tSensitive to noise in the training data\n",
    "May underfit if C is too small\t        May overfit if C is too large\n",
    "\n",
    "\n",
    "Hyperparameter:\n",
    "\n",
    "C\tRegularization parameter. Controls the trade-off between a smooth decision boundary and training accuracy.\t1.0\n",
    "kernel\tSpecifies the kernel type. Options: \"linear\", \"poly\", \"rbf\", \"sigmoid\", or \"precomputed\".\t\"rbf\"\n",
    "degree\tDegree of the polynomial kernel function (only for kernel='poly').\t3\n",
    "gamma\tKernel coefficient for \"rbf\", \"poly\", and \"sigmoid\". Options: \"scale\" (default) or \"auto\".\t\"scale\"\n",
    "coef0\tIndependent term in the kernel function (used in poly and sigmoid kernels).\t0.0\n",
    "shrinking\tWhether to use the shrinking heuristic for optimization.\tTrue\n",
    "probability\tWhether to enable probability estimates (slower and uses more memory).\tFalse\n",
    "tol\tTolerance for stopping criteria.\t1e-3\n",
    "max_iter\tHard limit on iterations within the solver. -1 means no limit.\t-1 (unlimited)\n",
    "class_weight\tWeights associated with classes. \"balanced\" automatically adjusts weights inversely proportional to class frequencies.\tNone\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "working:\n",
    "        K-Nearest Neighbors (KNN) is a simple, instance-based, and non-parametric algorithm used for classification and regression tasks.\n",
    "        In KNN, the input consists of the kk closest training examples in the feature space,\n",
    "        and the output is based on a majority vote (for classification) or the mean/median of the neighbors (for regression).\n",
    "\n",
    "\n",
    "Hyperparameter:\n",
    "        Number of Neighbors (kk):\n",
    "            Small kk: If kk is too small (e.g., k=1k=1), the model may be sensitive to noise and may overfit the training data.\n",
    "            Large kk: If kk is too large, the algorithm may become too generalized, meaning it will consider too many neighbors, including those that may not be relevant.\n",
    "\n",
    "        Distance Metric:\n",
    "            Euclidean Distance, Manhattan Distance, Hamming Distance:\n",
    "\n",
    "        Weight Function (Weighting of Neighbors):\n",
    "            In weighted KNN, different weights are assigned to neighbors based on their distance from the query point.\n",
    "\n",
    "\n",
    "        Algorithm (Search Method):\n",
    "            Brute-force:, Ball Tree, KD Tree\n",
    "\n",
    "The term “non-parametric” refers to not making any assumptions on the underlying data distribution\n",
    "\n",
    "The odd value of K should be preferred over even values in order to ensure that there are no ties in the voting.\n",
    "If the square root of a number of data points is even, then add or subtract 1 to it to make it odd\n",
    "\n",
    "In other words, the KNN algorithm can be applied  when the dependent variable is continuous. For regression problem statements,\n",
    "the predicted value is given by the average of the values of its k nearest neighbours.\n",
    "\n",
    "When the KNN algorithm gets the training data, it does not learn and make a model, it just stores the data\n",
    "\n",
    "k is high mode is underfit\n",
    "n_neighbors\tNumber of neighbors to consider for predictions.\t5\n",
    "weights\tWeight function used in prediction. Options: \"uniform\" (all neighbors equal) or \"distance\" (closer neighbors have higher influence).\t\"uniform\"\n",
    "algorithm\tAlgorithm to compute nearest neighbors. Options: \"auto\", \"ball_tree\", \"kd_tree\", \"brute\".\t\"auto\"\n",
    "leaf_size\tLeaf size for BallTree or KDTree. Affects speed and memory.\t30\n",
    "p\tPower parameter for the Minkowski distance metric: (\\text{distance} = (\\sum\tx_i - y_i\n",
    "metric\tDistance metric to use. Common options: \"minkowski\", \"euclidean\", \"manhattan\", \"chebyshev\".\t\"minkowski\"\n",
    "metric_params\tAdditional keyword arguments for the distance metric.\tNone\n",
    "n_jobs\tNumber of parallel jobs for neighbor search. -1 uses all available processors.\tNone\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## kmeans"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "working:\n",
    "\n",
    "\n",
    "Hyperparameter:\n",
    "    Number of Clusters (kk):\n",
    "        Too small kk: Clusters will be too large, potentially combining distinct groups into one, resulting in underfitting.\n",
    "        large kk: Clusters will be too small, possibly splitting a single group into multiple clusters, resulting in overfitting and capturing noise.\n",
    "\n",
    "    Initialization Method:\n",
    "        Random Initialization, k-means++ Initialization --> Default one\n",
    "\n",
    "\n",
    "    Distance Metric:\n",
    "        Euclidean Distance, Manhattan Distance, Cosine Similarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## GMM"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "    Parameters of a Gaussian:\n",
    "        Each Gaussian distribution is described by:\n",
    "            Mean (μ): The center of the cluster.\n",
    "            Covariance (Σ): The shape and orientation of the cluster. This accounts for the variance in different directions (important for non-spherical clusters).\n",
    "            Weight (π): The proportion of points belonging to that cluster in the overall dataset.\n",
    "    Expectation-Maximization (EM) Algorithm:\n",
    "        GMM uses the EM algorithm to find the parameters (mean, covariance, and weight) that best fit the data. It iteratively updates the cluster assignments and the parameters until convergence:\n",
    "            Expectation Step (E-step): Calculate the probability of each point belonging to each cluster based on current parameters.\n",
    "            Maximization Step (M-step): Update the parameters (mean, covariance, and weight) based on the current assignments to maximize the likelihood of the data.\n",
    "\n",
    "Steps in GMM Clustering:\n",
    "\n",
    "    Initialization: Randomly initialize the means, covariances, and weights of the Gaussians.\n",
    "    E-step: For each data point, compute the probability that it belongs to each Gaussian (cluster) using the current parameters.\n",
    "    M-step: Update the parameters (mean, covariance, and weight) of each Gaussian by maximizing the likelihood that the data comes from that distribution.\n",
    "    Convergence: Repeat the E-step and M-step until the parameters converge, meaning the clusters stabilize.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "replacement sampling method\n",
    "oob --> nothinh but the validation or test data\n",
    "Random’ in Random Forest refers to mainly two processes –\n",
    "\n",
    "    Random observations to grow each tree.\n",
    "    Random variables selected for splitting at each node.\n",
    "\n",
    "n_estimators\tNumber of trees in the forest.\t100\n",
    "criterion\tFunction to measure split quality. Options: Classification (\"gini\", \"entropy\", \"log_loss\")\n",
    "            Regression (\"squared_error\", \"absolute_error\", \"poisson\").\tClassification: \"gini\", Regression: \"squared_error\"\n",
    "max_depth\tMaximum depth of the tree. None means nodes are expanded until all leaves are pure.\tNone\n",
    "min_samples_split\tMinimum number of samples required to split an internal node.\t2\n",
    "min_samples_leaf\tMinimum number of samples required to be at a leaf node.\t1\n",
    "min_weight_fraction_leaf\tMinimum weighted fraction of the total sample weight required at a leaf node.\t0.0\n",
    "max_features\tNumber of features to consider when looking for the best split. Options: None, \"sqrt\", \"log2\",\n",
    "                or a fraction/int value.\tClassification: \"sqrt\", Regression: 1.0\n",
    "max_leaf_nodes\tMaximum number of leaf nodes. None means unlimited.\tNone\n",
    "min_impurity_decrease\tA node will split if impurity decrease is greater than or equal to this value.\t0.0\n",
    "bootstrap\tWhether bootstrap samples are used when building trees.\tTrue\n",
    "oob_score\tWhether to use out-of-bag samples to estimate the generalization score.\tFalse\n",
    "n_jobs\tNumber of jobs to run in parallel. -1 means use all processors.\tNone\n",
    "random_state\tSeed for reproducibility.\tNone\n",
    "verbose\tControls the verbosity of the tree building process.\t0\n",
    "warm_start\tWhether to reuse previous solution to add more estimators to the ensemble.\tFalse\n",
    "ccp_alpha\tComplexity parameter for Minimal Cost-Complexity Pruning.\t0.0\n",
    "max_samples\tIf bootstrap is True, this sets the number of samples drawn for each tree.\tNone\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "parameteric algo -->> linear and logistic reg, NB, -->> depend on data distribution\n",
    "non parameteric algo -->> knn,svm, randomforest, dicision tree -->> dont depend on data distribution\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "not to use rand.forest in below situation\n",
    "1. feature and target have linear relation -->> use linear reg, log reg\n",
    "2. data is noisy and sparse (RF likes many distict vlaues)\n",
    "3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "def dgt_root(n):\n",
    "    dgt_root = 0\n",
    "    while n > 0:\n",
    "        dgt_root = dgt_root + n % 10\n",
    "        n = n//10\n",
    "\n",
    "        if n ==0 and dgt_root > 9:\n",
    "            n = dgt_root\n",
    "            dgt_root = 0\n",
    "    return dgt_root\n",
    "print(dgt_root(38))\n",
    "arr = [5,8,4,9]\n",
    "def sum_dgt(arr):\n",
    "    s=0\n",
    "    for i in arr:\n",
    "        s = s + i\n",
    "    return dgt_root(s)\n",
    "print(sum_dgt(arr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLNLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
